import os
import json
import urllib.request
import urllib.error
import sys

# --- Configuration from Environment Variables ---
PROJECT_ID = os.environ.get("GCP_PROJECT_ID")
REGION = os.environ.get("GCP_REGION", "us-central1")
VERTEX_API_KEY = os.environ.get("VERTEX_API_KEY")
VERTEX_ENDPOINT_ID = os.environ.get("VERTEX_ENDPOINT_ID")
BASE_MODEL_ID = "gemini-2.0-flash-001"

# Default Prompt if not provided via args
DEFAULT_PROMPT = "éƒ¨ä¸‹ã‹ã‚‰ã€ãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ãŒä¸ŠãŒã‚‰ãªã„ã€ã¨ç›¸è«‡ã•ã‚Œã¾ã—ãŸã€‚ã©ã†å¯¾å¿œã—ã¾ã™ã‹ï¼Ÿ"

def call_api(model_resource_url, prompt, label):
    url = f"{model_resource_url}:streamGenerateContent?key={VERTEX_API_KEY}"
    
    payload = {
        "contents": [{"role": "user", "parts": [{"text": prompt}]}],
        "generationConfig": {"maxOutputTokens": 8192, "temperature": 0.7}
    }
    
    headers = {"Content-Type": "application/json"}
    data = json.dumps(payload).encode("utf-8")
    
    full_text = ""
    try:
        req = urllib.request.Request(url, data=data, headers=headers, method="POST")
        with urllib.request.urlopen(req) as response:
            full_resp = response.read().decode("utf-8")
            try:
                resp_list = json.loads(full_resp)
                for item in resp_list:
                    if "candidates" in item:
                        for cand in item["candidates"]:
                             if "content" in cand and "parts" in cand["content"]:
                                 for part in cand["content"]["parts"]:
                                     full_text += part.get("text", "")
            except Exception as e:
                return f"Error parsing JSON: {e}"
    except Exception as e:
        return f"Error ({label}): {e}"
    
    return full_text

import argparse
import re

# --- å®šé‡è©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯ ---

def evaluate_response(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å®šé‡æŒ‡æ¨™ã‚’è¨ˆç®—ã—ã¦è¿”ã™"""
    char_count = len(text)
    paragraphs = [p for p in text.split("\n") if p.strip()]
    paragraph_count = len(paragraphs)
    effective_char_count = len(text.replace(" ", "").replace("ã€€", "").replace("\n", "").replace("\r", ""))
    return {
        "char_count": char_count,
        "paragraph_count": paragraph_count,
        "effective_char_count": effective_char_count,
    }

def format_score_report(base_score, tuned_score):
    """ä¸¡ã‚¹ã‚³ã‚¢ã‚’æ¯”è¼ƒã—ã¦Markdownå½¢å¼ã®è©•ä¾¡ã‚µãƒãƒªãƒ¼ã‚’è¿”ã™"""
    rows = [
        ("æ–‡å­—æ•°", base_score["char_count"], tuned_score["char_count"]),
        ("æ®µè½æ•°", base_score["paragraph_count"], tuned_score["paragraph_count"]),
        ("å®Ÿè³ªæ–‡å­—æ•°", base_score["effective_char_count"], tuned_score["effective_char_count"]),
    ]

    table_lines = [
        "| æŒ‡æ¨™ | ğŸ”¹ ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ« | ğŸ”¸ ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ | å·®åˆ† |",
        "|---|---|---|---|",
    ]
    win_count = 0
    for label, base_val, tuned_val in rows:
        diff = tuned_val - base_val
        sign = "+" if diff >= 0 else ""
        mark = "âœ…" if diff > 0 else ("â–" if diff == 0 else "âš ï¸")
        if diff > 0:
            win_count += 1
        table_lines.append(f"| {label} | {base_val} | {tuned_val} | {sign}{diff} {mark} |")

    if win_count == len(rows):
        verdict = "**åˆ¤å®š:** ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®å›ç­”ãŒã™ã¹ã¦ã®æŒ‡æ¨™ã§ä¸Šå›ã£ã¦ã„ã¾ã™ã€‚"
    elif win_count > 0:
        verdict = f"**åˆ¤å®š:** ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒ {win_count}/{len(rows)} æŒ‡æ¨™ã§ä¸Šå›ã£ã¦ã„ã¾ã™ã€‚"
    else:
        verdict = "**åˆ¤å®š:** ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨åŒç­‰ã‹ãã‚Œä»¥ä¸Šã®çµæœã§ã™ã€‚ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®è¦‹ç›´ã—ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚"

    report = "## ğŸ“Š å®šé‡è©•ä¾¡ã‚µãƒãƒªãƒ¼\n\n"
    report += "\n".join(table_lines)
    report += f"\n\n{verdict}\n"
    return report

# --- LLM-as-Judge ãƒ­ã‚¸ãƒƒã‚¯ ---

JUDGE_CRITERIA = ["å®Ÿç”¨æ€§", "å…±æ„Ÿæ€§", "å°‚é–€æ€§"]

JUDGE_PROMPT_TEMPLATE = """\
ã‚ãªãŸã¯ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆã®å°‚é–€å®¶ã§ã™ã€‚
ä»¥ä¸‹ã®è³ªå•ã«å¯¾ã™ã‚‹å›ç­”ã‚’å³å¯†ã«è©•ä¾¡ã—ã€å¿…ãšJSONå½¢å¼ã®ã¿ã§è¿”ã—ã¦ãã ã•ã„ã€‚

## è³ªå•
{original_prompt}

## è©•ä¾¡å¯¾è±¡ã®å›ç­”
{response_text}

## è©•ä¾¡åŸºæº– (å„è»¸ 1ã€œ5 ç‚¹)
- å®Ÿç”¨æ€§: å…·ä½“çš„ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚„æ‰‹é †ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹
- å…±æ„Ÿæ€§: ç›¸è«‡è€…ã®æ„Ÿæƒ…ã‚’å—ã‘æ­¢ã‚ã€å¿ƒç†çš„å®‰å…¨æ€§ã‚’ç¢ºä¿ã—ã¦ã„ã‚‹ã‹
- å°‚é–€æ€§: ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆç†è«–ã‚„æ¥­ç•ŒçŸ¥è­˜ãŒåæ˜ ã•ã‚Œã¦ã„ã‚‹ã‹

## å‡ºåŠ›å½¢å¼
ä»¥ä¸‹ã®JSONã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚ä»–ã®ãƒ†ã‚­ã‚¹ãƒˆã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ã€‚
{{"å®Ÿç”¨æ€§": <1-5>, "å…±æ„Ÿæ€§": <1-5>, "å°‚é–€æ€§": <1-5>, "ã‚³ãƒ¡ãƒ³ãƒˆ": "<50å­—ä»¥å†…ã®ç·è©•>"}}
"""

def call_judge(response_text, original_prompt):
    """ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’å¯©åˆ¤ã¨ã—ã¦å‘¼ã³å‡ºã—ã€ã‚¹ã‚³ã‚¢ã‚’è¿”ã™"""
    if not VERTEX_API_KEY or not PROJECT_ID:
        return None, "VERTEX_API_KEY ã¾ãŸã¯ GCP_PROJECT_ID ãŒæœªè¨­å®šã§ã™ã€‚"

    judge_prompt = JUDGE_PROMPT_TEMPLATE.format(
        original_prompt=original_prompt,
        response_text=response_text,
    )
    base_url = (
        f"https://{REGION}-aiplatform.googleapis.com/v1beta1"
        f"/projects/{PROJECT_ID}/locations/{REGION}"
        f"/publishers/google/models/{BASE_MODEL_ID}"
    )
    url = f"{base_url}:streamGenerateContent?key={VERTEX_API_KEY}"
    payload = {
        "contents": [{"role": "user", "parts": [{"text": judge_prompt}]}],
        "generationConfig": {"maxOutputTokens": 512, "temperature": 0.1},
    }
    headers = {"Content-Type": "application/json"}
    data = json.dumps(payload).encode("utf-8")

    full_text = ""
    try:
        req = urllib.request.Request(url, data=data, headers=headers, method="POST")
        with urllib.request.urlopen(req) as response:
            resp_list = json.loads(response.read().decode("utf-8"))
            for item in resp_list:
                for cand in item.get("candidates", []):
                    for part in cand.get("content", {}).get("parts", []):
                        full_text += part.get("text", "")
    except Exception as e:
        return None, str(e)

    # JSONã‚’æŠ½å‡º
    match = re.search(r"\{.*\}", full_text, re.DOTALL)
    if not match:
        return None, f"JSONãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ: {full_text[:200]}"
    try:
        result = json.loads(match.group())
        return result, None
    except json.JSONDecodeError as e:
        return None, f"JSONãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e} / raw: {match.group()[:200]}"

def format_judge_report(base_judgment, tuned_judgment, base_error, tuned_error):
    """å¯©åˆ¤ã‚¹ã‚³ã‚¢ã‚’æ¯”è¼ƒã—ã¦Markdownå½¢å¼ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’è¿”ã™"""
    report = "## ğŸ§‘\u200dâš–ï¸ AIå¯©åˆ¤ã«ã‚ˆã‚‹è©•ä¾¡\n\n"

    def render_table(judgment, error):
        if error or judgment is None:
            return f"> è©•ä¾¡ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {error}\n"
        lines = [
            "| è»¸ | ã‚¹ã‚³ã‚¢ |",
            "|---|---|",
        ]
        total = 0
        for key in JUDGE_CRITERIA:
            score = judgment.get(key, "-")
            if isinstance(score, int):
                total += score
            lines.append(f"| {key} | {score}/5 |")
        lines.append(f"| **åˆè¨ˆ** | **{total}/15** |")
        comment = judgment.get("ã‚³ãƒ¡ãƒ³ãƒˆ", "")
        return "\n".join(lines) + (f"\n\n> {comment}" if comment else "")

    report += "### ğŸ”¹ ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«\n"
    report += render_table(base_judgment, base_error) + "\n\n"
    report += "### ğŸ”¸ ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«\n"
    report += render_table(tuned_judgment, tuned_error) + "\n\n"

    # åˆ¤å®š
    if base_judgment and tuned_judgment:
        base_total = sum(base_judgment.get(k, 0) for k in JUDGE_CRITERIA)
        tuned_total = sum(tuned_judgment.get(k, 0) for k in JUDGE_CRITERIA)
        diff = tuned_total - base_total
        sign = "+" if diff >= 0 else ""
        if diff > 0:
            verdict = f"**åˆ¤å®š:** ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒ {sign}{diff}ç‚¹ ä¸Šå›ã£ã¦ã„ã¾ã™ã€‚ âœ…"
        elif diff == 0:
            verdict = "**åˆ¤å®š:** ä¸¡ãƒ¢ãƒ‡ãƒ«ã¯åŒç‚¹ã§ã™ã€‚ â–"
        else:
            verdict = f"**åˆ¤å®š:** ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ãŒ {abs(diff)}ç‚¹ ä¸Šå›ã£ã¦ã„ã¾ã™ã€‚ âš ï¸"
        report += verdict + "\n"

    return report

def parse_arguments():
    parser = argparse.ArgumentParser(description="Run specific model comparison tasks.")
    parser.add_argument("prompt", nargs="?", help="The prompt to send to the models.")
    parser.add_argument("--mode", choices=["parse", "base", "tuned", "simultaneous", "evaluate", "judge"], default="simultaneous", help="Execution mode.")
    parser.add_argument("--body", help="Issue body content for parsing prompt.")
    parser.add_argument("--base-file", help="Base model result file path (for evaluate/judge mode).")
    parser.add_argument("--tuned-file", help="Tuned model result file path (for evaluate/judge mode).")
    parser.add_argument("--prompt-text", help="Original prompt text (for judge mode).")
    parser.add_argument("--output", default="score_result.md", help="Output file path for evaluate/judge mode.")
    return parser.parse_args()

def main():
    args = parse_arguments()

    # --- Mode: Evaluate (å®šé‡è©•ä¾¡ã®ã¿) ---
    if args.mode == "evaluate":
        base_file = args.base_file
        tuned_file = args.tuned_file
        if not base_file or not tuned_file:
            print("Error: --base-file ã¨ --tuned-file ãŒå¿…è¦ã§ã™ã€‚")
            sys.exit(1)
        with open(base_file, "r", encoding="utf-8") as f:
            base_text = f.read()
        with open(tuned_file, "r", encoding="utf-8") as f:
            tuned_text = f.read()
        base_score = evaluate_response(base_text)
        tuned_score = evaluate_response(tuned_text)
        report = format_score_report(base_score, tuned_score)
        output_path = args.output
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(report)
        print(report)
        return

    # --- Mode: Judge (LLM-as-Judge) ---
    if args.mode == "judge":
        base_file = args.base_file
        tuned_file = args.tuned_file
        original_prompt = args.prompt_text or DEFAULT_PROMPT
        if not base_file or not tuned_file:
            print("Error: --base-file ã¨ --tuned-file ãŒå¿…è¦ã§ã™ã€‚")
            sys.exit(1)
        with open(base_file, "r", encoding="utf-8") as f:
            base_text = f.read()
        with open(tuned_file, "r", encoding="utf-8") as f:
            tuned_text = f.read()
        print("ğŸ§‘â€âš–ï¸ ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®å›ç­”ã‚’æ¡ç‚¹ä¸­...")
        base_judgment, base_error = call_judge(base_text, original_prompt)
        print("ğŸ§‘â€âš–ï¸ ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®å›ç­”ã‚’æ¡ç‚¹ä¸­...")
        tuned_judgment, tuned_error = call_judge(tuned_text, original_prompt)
        report = format_judge_report(base_judgment, tuned_judgment, base_error, tuned_error)
        output_path = args.output
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(report)
        print(report)
        return

    # --- Mode: Parse Prompt from Issue Body ---
    if args.mode == "parse":
        content = args.body or os.environ.get("ISSUE_BODY")
        if not content:
             print("Error: Body content is required (via --body or ISSUE_BODY env var).")
             sys.exit(1)
        
        # Simple parsing for Issue Form (assuming formatting or direct text)
        # Issue Forms usually return key-value pairs or just sections.
        # We'll just take the body as is if it's simple, or try to find the prompt section.
        # For simplicity in this v1, assuming the user writes the prompt or the template logic works.
        # Actually, GitHub Issue Forms (yaml) put the content in the body.
        # If using the YAML template provided, the body will contain:
        # ### Prompt
        #
        # <user input>
        
        # content is already set above
        match = re.search(r"### Prompt\s*\n\s*(.*)", content, re.DOTALL)
        if match:
            print(match.group(1).strip())
        else:
            # Fallback: just return the whole body if pattern not found
            print(content.strip())
        return

    # --- Standard Execution ---
    if not VERTEX_API_KEY:
        print("Error: VERTEX_API_KEY environment variable not set.")
        sys.exit(1)

    prompt = args.prompt
    if not prompt:
        print("Error: prompt argument is required for model execution modes.")
        sys.exit(1)

    # 1. Base Model
    if args.mode in ["base", "simultaneous"]:
        base_url = f"https://{REGION}-aiplatform.googleapis.com/v1beta1/projects/{PROJECT_ID}/locations/{REGION}/publishers/google/models/{BASE_MODEL_ID}"
        base_res = call_api(base_url, prompt, "Base Model")
        
        print(f"### ğŸ”¹ Base Model ({BASE_MODEL_ID})")
        print(base_res)

    # 2. Tuned Model
    if args.mode in ["tuned", "simultaneous"]:
        tuned_url = f"https://{REGION}-aiplatform.googleapis.com/v1beta1/projects/{PROJECT_ID}/locations/{REGION}/endpoints/{VERTEX_ENDPOINT_ID}"
        tuned_res = call_api(tuned_url, prompt, "Tuned Model")
        
        print(f"### ğŸ”¸ Tuned Model (Fine-Tuned)")
        print(tuned_res)

if __name__ == "__main__":
    main()
